{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf\n",
      "<class 'sklearn.svm.classes.SVC'>\n",
      "[{'C': [1, 10, 100, 1000], 'kernel': ['linear']}, {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}]\n",
      "{'C': [1, 10, 100, 1000], 'kernel': ['linear']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be real number, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ab6420e98e99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mind_gs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlst_of_grids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mind_gs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf_hyper\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mind_gs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[1;31m#print(results)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-ab6420e98e99>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(a_clf, data, clf_hyper)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# EDIT: First param, M when subset by \"train_index\",\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m     \u001b[1;31m#      includes training X's.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;31m#      Second param, L when subset by \"train_index\",\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    266\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\svm\\libsvm.pyx\u001b[0m in \u001b[0;36msklearn.svm.libsvm.fit\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: must be real number, not list"
     ]
    }
   ],
   "source": [
    "# adapt this to run\n",
    "\n",
    "# 1. write a function to take a list or dictionary of clfs and hypers ie use logistic regression, each with 3 different sets of hyper parameters for each\n",
    "# 2. expand to include larger number of classifiers and hyperparmater settings\n",
    "# 3. find some simple data\n",
    "# 4. generate matplotlib plots that will assist in identifying the optimal clf and parampters settings\n",
    "# 5. Please set up your code to be run and save the results to the directory that its executed from\n",
    "# 6. Collaborate to get things\n",
    "# 7. Investigate grid search function\n",
    "\n",
    "\n",
    "# EDIT: array M includes the X's\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold  # EDIT: I had to import KFold\n",
    "M = np.array([[1, 2], [3, 4], [4, 5], [4, 5], [4, 5], [4, 5], [4, 5], [4, 5], [4, 5], [4, 5], [4, 5], [4, 5], [4, 5], [4, 5], [4, 5], [4, 5]])\n",
    "\n",
    "#EDIT: array L includes the Y's, they're all ones and as such is only for example (an ML algorithm would always predict 1).\n",
    "# So, I gave us some 0's too for Logistic Regression\n",
    "# L = np.ones(M.shape[0])\n",
    "L = np.random.choice([0, 1], size=(M.shape[0],), p=[1./3, 2./3])\n",
    "\n",
    "#EDIT: a single value, 5, to use for 5-fold (k-fold) cross validation\n",
    "n_folds = 5\n",
    "\n",
    "#EDIT: pack the arrays together into \"data\"\n",
    "data = (M, L, n_folds)\n",
    "\n",
    "#EDIT: Let's see what we have.\n",
    "#print(data)\n",
    "\n",
    "# data expanded\n",
    "M, L, n_folds = data\n",
    "kf = KFold(n_splits=n_folds)\n",
    "\n",
    "#print(kf)\n",
    "\n",
    "\"\"\"\n",
    "#EDIT: Show what is kf.split doing\n",
    "for ids, (train_index, test_index) in enumerate(kf.split(M, L)):\n",
    "    print(\"k fold = \", ids)\n",
    "    print(\"            train indexes\", train_index)\n",
    "    print(\"            test indexes\", test_index)\n",
    "\"\"\"\n",
    "\n",
    "#EDIT: A function, \"run\", to run all our classifiers against our data.\n",
    "\n",
    "\n",
    "def run(a_clf, data, clf_hyper={}):\n",
    "  M, L, n_folds = data  # EDIT: unpack the \"data\" container of arrays\n",
    "  kf = KFold(n_splits=n_folds)  # JS: Establish the cross validation\n",
    "  ret = {}  # JS: classic explicaiton of results\n",
    "\n",
    "  # EDIT: We're interating through train and test indexes by using kf.split\n",
    "  for ids, (train_index, test_index) in enumerate(kf.split(M, L)):\n",
    "                                                                   #      from M and L.\n",
    "                                                                   #      We're simply splitting rows into train and test rows\n",
    "                                                                   #      for our five folds.\n",
    "\n",
    "    # JS: unpack paramters into clf if they exist   #EDIT: this gives all keyword arguments except\n",
    "    clf = a_clf(**clf_hyper)\n",
    "    #      for those corresponding to a formal parameter\n",
    "    #      in a dictionary.\n",
    "\n",
    "    # EDIT: First param, M when subset by \"train_index\",\n",
    "    clf.fit(M[train_index], L[train_index])\n",
    "    #      includes training X's.\n",
    "    #      Second param, L when subset by \"train_index\",\n",
    "    #      includes training Y.\n",
    "\n",
    "    # EDIT: Using M -our X's- subset by the test_indexes,\n",
    "    pred = clf.predict(M[test_index])\n",
    "    #      predict the Y's for the test rows.\n",
    "\n",
    "    ret[ids] = {'clf': clf,  # EDIT: Create arrays of\n",
    "                'train_index': train_index,\n",
    "                'test_index': test_index,\n",
    "                'accuracy': accuracy_score(L[test_index], pred)}\n",
    "  return ret\n",
    "\n",
    "#Use run function with a list and a for loop\n",
    "\n",
    "\n",
    "clfsList = [RandomForestClassifier, LogisticRegression]\n",
    "\n",
    "gs = {'rf': {'model': SVC,\n",
    "             'param_grid': [{'C': [1, 10, 100, 1000],\n",
    "                             'kernel': ['linear']},\n",
    "                            {'C': [1, 10, 100, 1000],\n",
    "                             'gamma': [0.001, 0.0001],\n",
    "                             'kernel': ['rbf']}]\n",
    "             }\n",
    "     }\n",
    "\n",
    "#for clfs in clfsList:\n",
    "#    results = run(clfs, data, clf_hyper={})\n",
    "#    print(results)\n",
    "\n",
    "for name in gs.keys():\n",
    "    print (name)\n",
    "    print(gs[name]['model'])\n",
    "    print(gs[name]['param_grid'])\n",
    "\n",
    "    model = gs[name]['model']\n",
    "    lst_of_grids = gs[name]['param_grid']\n",
    "    for i in range(len(lst_of_grids)):\n",
    "        ind_gs = lst_of_grids[i]\n",
    "        print(ind_gs)\n",
    "        results = run(model, data, clf_hyper=ind_gs)\n",
    "        #print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
